---
title: "P8105 Homework 3"
output: github_document
date: "November 15th, 2022"
---

```{r setup, include = FALSE}
library(tidyverse)
library(p8105.datasets)

knitr::opts_chunk$set(
  warning = FALSE, 
  message = FALSE,
  fig.dim = c(12, 7))

theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

## Part 1: 

## Part 2: U.S. Homicide Rates

Let's load the raw homicide data and inspect it.

```{r load data}
homicide_data = read_csv("data/homicide-data.csv") %>% 
  janitor::clean_names()

homicide_data
```

There are `r nrow(homicide_data)` observations of `r ncol(homicide_data)` variables containing information about homicides in 50 cities across the US. The data contains the date, victim information (name, age, sex, race), location (city, state, latitude, longitude), and status of the case.

Next, let's summarize the data within cities to obtain the total number of homicides and the number of unsolved homicides.

```{r clean data}
homicide_summary = homicide_data %>% 
  mutate(
    state = str_to_upper(state),
    city_state = str_c(city, state, sep = ", "), 
    status = ifelse(
      disposition == "Closed without arrest" | disposition == "Open/No arrest", "Solved", "Unsolved")) %>% 
  group_by(city_state) %>% 
  summarise(n_unsolved = sum(status == "Unsolved"), 
            n_total = n()) 

homicide_summary
```

Next, for the city of Baltimore, MD, we will use the `prop.test` function to estimate the proportion of homicides that are unsolved and its confidence interval.

```{r estimates for Baltimore}
baltimore_df = homicide_summary %>% 
  filter(city_state == "Baltimore, MD") 

x = baltimore_df %>% pull(n_unsolved)
n = baltimore_df %>% pull(n_total)
  
baltimore_est = prop.test(x, n) 

baltimore_est %>% 
  broom::tidy() %>% 
  select(estimate, conf.low, conf.high) %>% 
  mutate_all(~ . * 100) %>% 
  rename("Estimated proportion of unsolved homicides" = estimate,
         "Lower confidence limit" = conf.low, 
         "Upper confidence limit" = conf.high)
```

We can see that the estimated proportion of of unsolved homicides in Baltimore is `r baltimore_est %>% broom::tidy() %>% pull(estimate)*100`%. We are 95% confident that the proportion of unsolved homicides in Baltimore is between `r baltimore_est %>% broom::tidy() %>% pull(conf.low)*100`% and `r baltimore_est %>% broom::tidy() %>% pull(conf.high)*100`%.

Next, we want use `map` to apply the `prop.test` function to estimate the proportion of unsolved homicides in all cities in our dataset.

```{r mapping prop.test}
unsolved_homicides = homicide_summary %>% 
  mutate(
    prop_test = map2(.x = n_unsolved, .y = n_total, ~prop.test(x = .x, n = .y) %>% 
                       broom::tidy())) %>% 
  unnest(prop_test) %>% 
  select(city_state:estimate, conf.low, conf.high)

unsolved_homicides
```

The resulting `unsolved_homicides` dataframe contains `r nrow(unsolved_homicides)` observations of `r ncol(unsolved_homicides)` variables, providing number of unsolved and total homicides, and the estimated proportion of unsolved homicides, along with their 95% CIs which come from mapping the `prop.test` function to all the cities.

Finally, we will create a plot of the estimated proportions of unsolved homicides in each city, along with their confidence intervals.

```{r plot}
unsolved_homicides %>% 
  mutate(city_state = fct_reorder(city_state, estimate)) %>% 
  ggplot(aes(x = city_state, y = estimate, colour = estimate)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high, width = .3)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1), 
        legend.position = "none") + 
  labs(
    x = "Location",
    y = "Estimated proportion of unsolved homicides",
    title = "Proportion of homicides that are unsolved in 50 U.S. cities, 2007-2017",
  )
```

## Part 3: Simulating parameters that affect power

A common question when designing experiments is whether a false null hypothesis will be rejected, or what the power of a study will be. Power depends on the sample size of our study, effect size, and error variance. In this problem, we will conduct a simulation to explore power in a one-sample t-test. 

First, we will write a function that estimates the mean and standard deviation from a normal distribution. We will fix the $n = 30$, $\sigma = 5$, and $\mu = 0$.  

```{r function}
sim_mean_sd = function(n = 30, mu = 0, sigma = 5) {
  
  sim_data = tibble(
    x = rnorm(n, mean = mu, sd = sigma),
  )
  
  sim_data %>% 
    summarise(
      mu_hat = mean(x),
      sigma_hat = sd(x)
    )
}

```

Next, we will generate 5000 datasets from this model

```{r}

```

