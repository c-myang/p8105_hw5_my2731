---
title: "P8105 Homework 3"
output: github_document
date: "November 16th, 2022"
---

```{r setup, include = FALSE}
library(tidyverse)
library(p8105.datasets)

knitr::opts_chunk$set(
  warning = FALSE, 
  message = FALSE,
  fig.dim = c(12, 7))

theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

## Part 1: Wrangling longitudinal study data

This zip file contains data from a longitudinal study that included a control arm and an experimental arm. Data for each participant is included in a separate file, and file names include the subject ID and arm. 

The code chunk below imports the data in individual spreadsheets contained in `./long_data/`. To do this, I create a dataframe that includes the list of all files in that directory and the complete path to each file. As a next step, I `map` over paths and import data using the `read_csv` function. Finally, I `unnest` the result of `map`.

```{r load longitudinal data}
full_df = tibble(
  files = list.files("./long_data"),
  path = str_c("./long_data/", files)) %>% 
  mutate(data = map(path, read_csv)) %>% 
  unnest()
```

The result of the previous code chunk isn't tidy -- data are wide rather than long, and some important variables are included as parts of others. The code chunk below tides the data using string manipulations on the file, converting from wide to long, and selecting relevant variables.

```{r}
tidy_df = full_df %>% 
  mutate(
    files = str_replace(files, ".csv", ""),
    group = str_sub(files, 1, 3)) %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    values_to = "outcome",
    names_prefix = "week_") %>% 
  mutate(week = as.numeric(week)) %>% 
  select(group, subj = files, week, outcome)
```

Finally, the code chunk below creates a plot showing individual data, faceted by group. 

```{r}
tidy_df %>% 
  ggplot(aes(x = week, y = outcome, group = subj, color = group)) + 
  geom_point() + 
  geom_path() + 
  facet_grid(~group) 
```

This plot suggests high within-subject correlation -- subjects who start above average end up above average, and those that start below average end up below average. Subjects in the control group generally don't change over time, but those in the experiment group increase their outcome in a roughly linear way. 

## Part 2: U.S. Homicide Rates

The Washington Post has gathered data on homicides in 50 large U.S. cities. Let's load the raw homicide data and inspect it.

```{r load homicide data}
homicide_data = read_csv("data/homicide-data.csv") %>% 
  janitor::clean_names()

homicide_data
```

There are `r nrow(homicide_data)` observations of `r ncol(homicide_data)` variables containing information about homicides in 50 cities across the US. The data contains the date, victim information (name, age, sex, race), location (city, state, latitude, longitude), and status of the case.

Next, let's tidy the data by creating a city_state variable, and changing wrongly entered data entries (notably, changing Tulsa, AL to Tulsa, OK). We will then summarise the data within cities to obtain the total number of homicides and the number of unsolved homicides.

```{r clean data}
homicide_summary = homicide_data %>% 
  mutate(
    state = str_to_upper(state),
    state = ifelse(city == "Tulsa" & state == "AL", "OK", state),
    city_state = str_c(city, state, sep = ", "), 
    status = ifelse(
      disposition == "Closed without arrest" | disposition == "Open/No arrest", "Solved", "Unsolved")) %>% 
  group_by(city_state) %>% 
  summarise(n_unsolved = sum(status == "Unsolved"), 
            n_total = n()) 

homicide_summary
```

Next, for the city of Baltimore, MD, we will use the `prop.test` function to estimate the proportion of homicides that are unsolved and its confidence interval.

```{r estimates for Baltimore}
baltimore_df = homicide_summary %>% 
  filter(city_state == "Baltimore, MD") 

x = baltimore_df %>% pull(n_unsolved)
n = baltimore_df %>% pull(n_total)
  
baltimore_est = prop.test(x, n) 

baltimore_est %>% 
  broom::tidy() %>% 
  select(estimate, conf.low, conf.high) %>% 
  mutate_all(~ . * 100) %>% 
  rename("Estimated proportion of unsolved homicides" = estimate,
         "Lower confidence limit" = conf.low, 
         "Upper confidence limit" = conf.high)
```

We can see that the estimated proportion of of unsolved homicides in Baltimore is `r baltimore_est %>% broom::tidy() %>% pull(estimate)*100`%. We are 95% confident that the proportion of unsolved homicides in Baltimore is between `r baltimore_est %>% broom::tidy() %>% pull(conf.low)*100`% and `r baltimore_est %>% broom::tidy() %>% pull(conf.high)*100`%.

Next, we want use `map2` to apply the `prop.test` function to estimate the proportion of unsolved homicides in all cities in our dataset.

```{r mapping prop.test}
unsolved_homicides = homicide_summary %>% 
  mutate(
    prop_test = map2(.x = n_unsolved, .y = n_total, ~prop.test(x = .x, n = .y) %>% 
                       broom::tidy())) %>% 
  unnest(prop_test) %>% 
  select(city_state:estimate, conf.low, conf.high)

unsolved_homicides
```

The resulting `unsolved_homicides` dataframe contains `r nrow(unsolved_homicides)` observations of `r ncol(unsolved_homicides)` variables, providing number of unsolved and total homicides, and the estimated proportion of unsolved homicides, along with their 95% CIs which come from mapping the `prop.test` function to all the cities.

Finally, we will create a plot of the estimated proportions of unsolved homicides in each city, along with their confidence intervals.

```{r plot}
unsolved_homicides %>% 
  mutate(city_state = fct_reorder(city_state, estimate)) %>% 
  ggplot(aes(x = city_state, y = estimate, colour = estimate)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high, width = .3)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1), 
        legend.position = "none") + 
  labs(
    x = "Location",
    y = "Estimated proportion of unsolved homicides",
    title = "Proportion of homicides that are unsolved in 50 U.S. cities, 2007-2017",
  )
```

## Part 3: Simulating parameters that affect power

A common question when designing experiments is whether a false null hypothesis will be rejected, or what the power of a study will be. Power depends on the sample size of our study, effect size, and error variance. In this problem, we will conduct a simulation to explore power in a one-sample t-test. 

First, we will write a function that generates a random sample from a normal distribution and outputs the results for a one-sample t-test of $H_0: \mu = 0$ for $\alpha = 0.05$ on the sample mean. We will fix the $n = 30$, $\sigma = 5$, and allow the $\mu$ to vary.

```{r sim_t_test function}
sim_t_test = function(true_mean) {
  
  sample = rnorm(n = 30, sd = 5, mean = true_mean)
  
  test_results = t.test(sample, mu = true_mean)
  
  test_results %>% 
    broom::tidy()
}
```

We will map the `sim_t_test` function 5000 times, setting the true $\mu = 0$, and saving the estimated mean and p-value from the t-test results.

```{r}
sim_df = expand_grid(
  true_mean = 0,
  iterate = 1:5000) %>% 
  mutate(ttest_df = map(true_mean, sim_t_test)) %>% 
  unnest(ttest_df) %>% 
  select(true_mean:estimate, p.value)

sim_df
```
The resulting dataset has `r nrow(sim_df)` iterations of estimates for $\hat{\mu}$ generated from a normal distribution and p-values for one-sample t-test for a true mean of $\mu = 0$. We can see that the $\hat{\mu}$ estimates hover around 0. For $\hat{\mu}$ estimates that are far from 0, the corresponding p-value is less than 0.05, indicating a rejection of the null hypothesis.

Now, we want to see how often $H_0$ is rejected when we increase $\mu$. For this, we will iterate on 5000 random samples of a normal distribution for $\mu= \{1,2,3,4,5,6\}$. We will also create a `reject_h0` variable to indicate whether the null hypothesis of $H_0: \mu = true mean$ was rejected based on the `p.value`.

```{r}
power_df = expand_grid(
  true_mean = 1:6,
  iterate = 1:5000) %>% 
  mutate(ttest_df = map(true_mean, sim_t_test)) %>% 
  unnest(ttest_df) %>% 
  select(true_mean:estimate, p.value) %>% 
  mutate(reject_h0 = ifelse(p.value < 0.05, "Null rejected", "Null not rejected"))

power_df
```

The resulting dataset has `r nrow(power_df)` iterations for true mean values ranging from 1 to 6, estimating the $\hat{\mu}$ generated from a normal distribution and p-values for one-sample t-test. We can see that the $\hat{\mu}$ estimates hover around the true mean value, and for $\hat{\mu}$ estimates that are far from $\mu$, the corresponding p-value is less than 0.05.

Let's plot the proportion of times the null was rejected, also known as the power of the test according to the true effect size $\mu$. 

```{r}
summary_power = power_df %>% 
  group_by(true_mean) %>% 
  summarise(prop_reject = sum(reject_h0 == "Null rejected")/n())

summary_power %>% 
  ggplot(aes(x = true_mean, y = prop_reject)) + 
  geom_point(colour = "purple", size = 2) + 
  labs(
    x = "True mean",
    y = "Power",
    title = "Power versus effect size for a 1-sample t-test"
  )
```

Notably, we see that as the effect size $\mu$ increases, the power increases.

Finally, we can plot the average estimates of $\hat{\mu}$ against the true $\mu$, for samples where the null hypothesis was rejected versus not rejected.

```{r}
power_df %>% 
  group_by(true_mean, reject_h0) %>% 
  summarise(mean_est = mean(estimate, na.rm = TRUE)) %>% 
  mutate(reject_h0 = fct_relevel(c("Null rejected", "Null not rejected"))) %>% 
  ggplot(aes(x = true_mean, y = mean_est, colour = reject_h0)) + 
  geom_point() + 
  labs(
    x = "True mean",
    y = "Estimated mean",
    title = "Estimated vs. true mean for samples where the null was rejected vs. not rejected", 
    color = ""
  )
```

Based on the plot, we can see that for samples where the null hypothesis was rejected, average $\hat{\mu}$Ì‚ estimates across tests for which the null is rejected is not equal to the true value of $\mu$. This is in accordance to what we would expect, as estimated means which deviate substantially from the true mean will lead to a rejection of the null hypothesis.

